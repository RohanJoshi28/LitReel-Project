{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5jj07NtV8rI",
        "outputId": "7c447e18-2713-41ab-e1a6-5dba701930e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'stories_shorts'...\n",
            "remote: Enumerating objects: 923, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 923 (delta 3), reused 3 (delta 3), pack-reused 918 (from 1)\u001b[K\n",
            "Receiving objects: 100% (923/923), 24.69 MiB | 13.64 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ]
        }
      ],
      "source": [
        "TOKEN=\"\"#FILL IN API TOKEN HERE\n",
        "#!git clone data repo here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdNPTYSsWZrX",
        "outputId": "5d04e769-a4db-4d91-d5ba-a882bd0b292e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "Using device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "Final dataset sizes (combined, no dropping):\n",
            "TRAIN: 116171 | AEM: 11907 | CR4: 104264\n",
            "VAL:   14521 | AEM: 1488 | CR4: 13033\n",
            "TEST:  14523 | AEM: 1489 | CR4: 13034\n",
            "Fitting quantile transformer (output_distribution='normal')...\n",
            "\n",
            "==============================\n",
            "ðŸ”¥ TRAINING MODEL\n",
            "==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 0:   0%|                                                             | 0/7261 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7261/7261 [03:01<00:00, 40.10it/s]\n",
            "Validating:   0%|                                                           | 0/908 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 908/908 [00:07<00:00, 120.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00 | TrainLoss=6706.7555 | ValLoss=0.50032 | ValMSE_norm=0.50046 | ValMSE_orig=0.00520 | Spearman_orig=0.7988\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1:   0%|                                                             | 0/7261 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7261/7261 [03:00<00:00, 40.27it/s]\n",
            "Validating:   0%|                                                           | 0/908 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 908/908 [00:07<00:00, 115.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 | TrainLoss=4338.9541 | ValLoss=0.48581 | ValMSE_norm=0.48601 | ValMSE_orig=0.00530 | Spearman_orig=0.8130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 2:   0%|                                                             | 0/7261 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7261/7261 [02:59<00:00, 40.49it/s]\n",
            "Validating:   0%|                                                           | 0/908 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 908/908 [00:07<00:00, 121.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 02 | TrainLoss=2802.7443 | ValLoss=0.47769 | ValMSE_norm=0.47786 | ValMSE_orig=0.00585 | Spearman_orig=0.8334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 3:   0%|                                                             | 0/7261 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7261/7261 [02:59<00:00, 40.52it/s]\n",
            "Validating:   0%|                                                           | 0/908 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 908/908 [00:07<00:00, 125.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 03 | TrainLoss=1727.0115 | ValLoss=0.47587 | ValMSE_norm=0.47604 | ValMSE_orig=0.00534 | Spearman_orig=0.8376\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 4:   0%|                                                             | 0/7261 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7261/7261 [02:59<00:00, 40.45it/s]\n",
            "Validating:   0%|                                                           | 0/908 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 908/908 [00:07<00:00, 118.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 04 | TrainLoss=1294.9092 | ValLoss=0.45468 | ValMSE_norm=0.45486 | ValMSE_orig=0.00533 | Spearman_orig=0.8461\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 5:   0%|                                                             | 0/7261 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7261/7261 [03:00<00:00, 40.19it/s]\n",
            "Validating:   0%|                                                           | 0/908 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 908/908 [00:07<00:00, 116.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 05 | TrainLoss=1036.7859 | ValLoss=0.43637 | ValMSE_norm=0.43656 | ValMSE_orig=0.00535 | Spearman_orig=0.8584\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 6:   0%|                                                             | 0/7261 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7261/7261 [03:00<00:00, 40.31it/s]\n",
            "Validating:   0%|                                                           | 0/908 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 908/908 [00:07<00:00, 121.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 06 | TrainLoss=806.9815 | ValLoss=0.43775 | ValMSE_norm=0.43794 | ValMSE_orig=0.00506 | Spearman_orig=0.8614\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 7:   0%|                                                             | 0/7261 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7261/7261 [02:57<00:00, 40.86it/s]\n",
            "Validating:   0%|                                                           | 0/908 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 908/908 [00:07<00:00, 121.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 07 | TrainLoss=693.6636 | ValLoss=0.43151 | ValMSE_norm=0.43169 | ValMSE_orig=0.00506 | Spearman_orig=0.8653\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 8:   0%|                                                             | 0/7261 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7261/7261 [02:59<00:00, 40.49it/s]\n",
            "Validating:   0%|                                                           | 0/908 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 908/908 [00:07<00:00, 119.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 08 | TrainLoss=627.1142 | ValLoss=0.43441 | ValMSE_norm=0.43460 | ValMSE_orig=0.00511 | Spearman_orig=0.8635\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 9:   0%|                                                             | 0/7261 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7261/7261 [02:59<00:00, 40.42it/s]\n",
            "Validating:   0%|                                                           | 0/908 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 908/908 [00:07<00:00, 117.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 09 | TrainLoss=549.5801 | ValLoss=0.43382 | ValMSE_norm=0.43401 | ValMSE_orig=0.00516 | Spearman_orig=0.8639\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 10:   0%|                                                            | 0/7261 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7261/7261 [02:58<00:00, 40.60it/s]\n",
            "Validating:   0%|                                                           | 0/908 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 908/908 [00:07<00:00, 120.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 | TrainLoss=501.6169 | ValLoss=0.43385 | ValMSE_norm=0.43404 | ValMSE_orig=0.00509 | Spearman_orig=0.8668\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 11:   0%|                                                            | 0/7261 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7261/7261 [03:00<00:00, 40.29it/s]\n",
            "Validating:   0%|                                                           | 0/908 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 908/908 [00:07<00:00, 121.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 | TrainLoss=496.1463 | ValLoss=0.43187 | ValMSE_norm=0.43206 | ValMSE_orig=0.00498 | Spearman_orig=0.8653\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 12:   0%|                                                            | 0/7261 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7261/7261 [03:00<00:00, 40.31it/s]\n",
            "Validating:   0%|                                                           | 0/908 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 908/908 [00:07<00:00, 121.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 | TrainLoss=449.3388 | ValLoss=0.42752 | ValMSE_norm=0.42771 | ValMSE_orig=0.00499 | Spearman_orig=0.8658\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 13:   0%|                                                            | 0/7261 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7261/7261 [02:59<00:00, 40.39it/s]\n",
            "Validating:   0%|                                                           | 0/908 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 908/908 [00:07<00:00, 116.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 | TrainLoss=443.9032 | ValLoss=0.42287 | ValMSE_norm=0.42306 | ValMSE_orig=0.00496 | Spearman_orig=0.8673\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 14:   0%|                                                            | 0/7261 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:323: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7261/7261 [03:00<00:00, 40.29it/s]\n",
            "Validating:   0%|                                                           | 0/908 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 908/908 [00:07<00:00, 121.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14 | TrainLoss=432.2257 | ValLoss=0.42487 | ValMSE_norm=0.42507 | ValMSE_orig=0.00495 | Spearman_orig=0.8669\n",
            "\n",
            "ðŸŽ‰ Saved best_model.pt + tokenizer + quantile transformer\n",
            "\n",
            "================ TEST RESULTS ================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating:   0%|                                                           | 0/908 [00:00<?, ?it/s]/tmp/ipython-input-305107645.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_amp):\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 908/908 [00:07<00:00, 118.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST MSE (norm):    0.45738276839256287\n",
            "TEST Spearman (norm): 0.8546728645786885\n",
            "TEST MSE (orig):    0.005438676103949547\n",
            "TEST Spearman (orig): 0.8546871886628019\n",
            "\n",
            "Running TF-IDF baseline...\n",
            "Baseline TF-IDF MSE (norm space): 0.6292091699258577\n",
            "Saved TF-IDF vectorizer â†’ tfidf_vectorizer.pkl\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.stats import spearmanr\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import nltk\n",
        "import joblib\n",
        "from torch.cuda.amp import autocast  # <-- AMP for CUDA\n",
        "\n",
        "nltk.download(\"wordnet\")\n",
        "from nltk.corpus import wordnet  # noqa: F401 (placeholder for future augmentation)\n",
        "\n",
        "# ============================================================\n",
        "# DEVICE & AMP SETUP  (FORCE CUDA)\n",
        "# ============================================================\n",
        "if not torch.cuda.is_available():\n",
        "    raise SystemError(\"CUDA is not available. Please enable a GPU runtime (e.g., Colab: Runtime â†’ Change runtime type â†’ GPU).\")\n",
        "\n",
        "device = \"cuda\"\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Using device:\", device)\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "torch.backends.cudnn.benchmark = True\n",
        "use_amp = True  # mixed precision on GPU\n",
        "\n",
        "# ============================================================\n",
        "# SPLIT CR4 INTO TRAIN / VAL / TEST (80/10/10)\n",
        "# ============================================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "cr4_full = pd.read_csv(\"data/cr4/CR4NarrEmote_t1Yes.csv\", low_memory=False)\n",
        "cr4_full = cr4_full.rename(columns={\"passage\": \"text\", \"EMO_arousal\": \"arousal\"})[[\"text\", \"arousal\"]]\n",
        "cr4_full[\"arousal\"] = pd.to_numeric(cr4_full[\"arousal\"], errors=\"coerce\")\n",
        "cr4_full = cr4_full.dropna()\n",
        "\n",
        "# 80% train, 20% temp\n",
        "cr4_train, cr4_temp = train_test_split(\n",
        "    cr4_full,\n",
        "    test_size=0.20,\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# split remaining 20% into 10% val, 10% test\n",
        "cr4_val, cr4_test = train_test_split(\n",
        "    cr4_temp,\n",
        "    test_size=0.50,\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# LOAD TALES_VA AND RESPLIT TO 80/10/10\n",
        "# ============================================================\n",
        "aem_train_raw = pd.read_csv(\"data/tales_va/train.csv\")[[\"text\", \"A_EWE\"]]\n",
        "aem_val_raw = pd.read_csv(\"data/tales_va/val.csv\")[[\"text\", \"A_EWE\"]]\n",
        "aem_test_raw = pd.read_csv(\"data/tales_va/test.csv\")[[\"text\", \"A_EWE\"]]\n",
        "\n",
        "aem_full = pd.concat([aem_train_raw, aem_val_raw, aem_test_raw], ignore_index=True)\n",
        "aem_full = aem_full.rename(columns={\"A_EWE\": \"arousal\"})\n",
        "aem_full[\"arousal\"] = pd.to_numeric(aem_full[\"arousal\"], errors=\"coerce\")\n",
        "aem_full = aem_full.dropna()\n",
        "\n",
        "train_aem, temp_aem = train_test_split(\n",
        "    aem_full,\n",
        "    test_size=0.20,\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        ")\n",
        "val_aem, test_aem = train_test_split(\n",
        "    temp_aem,\n",
        "    test_size=0.50,\n",
        "    random_state=42,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# COMBINE DATASETS WITHOUT DROPPING ANYTHING (OTHER THAN NaNs)\n",
        "# ============================================================\n",
        "train_aem[\"source\"] = \"aem\"\n",
        "val_aem[\"source\"] = \"aem\"\n",
        "test_aem[\"source\"] = \"aem\"\n",
        "\n",
        "cr4_train[\"source\"] = \"cr4\"\n",
        "cr4_val[\"source\"] = \"cr4\"\n",
        "cr4_test[\"source\"] = \"cr4\"\n",
        "\n",
        "train_df = pd.concat([train_aem, cr4_train], ignore_index=True)\n",
        "val_df = pd.concat([val_aem, cr4_val], ignore_index=True)\n",
        "test_df = pd.concat([test_aem, cr4_test], ignore_index=True)\n",
        "\n",
        "print(\"Final dataset sizes (combined, no dropping):\")\n",
        "print(\n",
        "    \"TRAIN:\", len(train_df),\n",
        "    \"| AEM:\", (train_df[\"source\"] == \"aem\").sum(),\n",
        "    \"| CR4:\", (train_df[\"source\"] == \"cr4\").sum(),\n",
        ")\n",
        "print(\n",
        "    \"VAL:  \", len(val_df),\n",
        "    \"| AEM:\", (val_df[\"source\"] == \"aem\").sum(),\n",
        "    \"| CR4:\", (val_df[\"source\"] == \"cr4\").sum(),\n",
        ")\n",
        "print(\n",
        "    \"TEST: \", len(test_df),\n",
        "    \"| AEM:\", (test_df[\"source\"] == \"aem\").sum(),\n",
        "    \"| CR4:\", (test_df[\"source\"] == \"cr4\").sum(),\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# NORMALIZE TARGETS (NO DATA LEAKAGE) - TO ~NORMAL\n",
        "# ============================================================\n",
        "print(\"Fitting quantile transformer (output_distribution='normal')...\")\n",
        "qt = QuantileTransformer(\n",
        "    n_quantiles=200,\n",
        "    output_distribution=\"normal\",\n",
        "    random_state=42,\n",
        ")\n",
        "qt.fit(train_df[\"arousal\"].values.reshape(-1, 1))\n",
        "\n",
        "train_df[\"arousal_norm\"] = qt.transform(train_df[\"arousal\"].values.reshape(-1, 1)).ravel()\n",
        "val_df[\"arousal_norm\"] = qt.transform(val_df[\"arousal\"].values.reshape(-1, 1)).ravel()\n",
        "test_df[\"arousal_norm\"] = qt.transform(test_df[\"arousal\"].values.reshape(-1, 1)).ravel()\n",
        "\n",
        "# ============================================================\n",
        "# AUGMENTATION (PLACEHOLDER)\n",
        "# ============================================================\n",
        "def augment(text: str, prob: float = 0.2) -> str:\n",
        "    \"\"\"\n",
        "    Simple placeholder for text augmentation.\n",
        "    Currently a no-op, but respects probability and is safe on CPU/GPU.\n",
        "    \"\"\"\n",
        "    if prob <= 0.0:\n",
        "        return text\n",
        "    if random.random() > prob:\n",
        "        return text\n",
        "    # TODO: add actual augmentation (e.g., synonym replacement using wordnet)\n",
        "    return text\n",
        "\n",
        "# ============================================================\n",
        "# DATASET + DATALOADERS\n",
        "# ============================================================\n",
        "# Use a smaller MiniLM model\n",
        "MINILM_MODEL_NAME = \"nreimers/MiniLM-L6-H384-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MINILM_MODEL_NAME)\n",
        "\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, augment_prob: float = 0.2):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.augment_prob = augment_prob\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        row = self.df.iloc[idx]\n",
        "        text = augment(row[\"text\"], prob=self.augment_prob)\n",
        "\n",
        "        enc = tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=256,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(float(row[\"arousal_norm\"]), dtype=torch.float32),\n",
        "        }\n",
        "\n",
        "\n",
        "# Weighted sampling for train set with ~75% CR4, 25% AEM\n",
        "train_sources = train_df[\"source\"].values\n",
        "n_aem = (train_sources == \"aem\").sum()\n",
        "n_cr4 = (train_sources == \"cr4\").sum()\n",
        "\n",
        "target_frac_aem = 0.25\n",
        "target_frac_cr4 = 0.75\n",
        "\n",
        "weights = np.where(\n",
        "    train_sources == \"aem\",\n",
        "    target_frac_aem / n_aem,\n",
        "    target_frac_cr4 / n_cr4,\n",
        ")\n",
        "weights = torch.DoubleTensor(weights)\n",
        "\n",
        "train_sampler = WeightedRandomSampler(\n",
        "    weights=weights,\n",
        "    num_samples=len(weights),\n",
        "    replacement=True,\n",
        ")\n",
        "\n",
        "train_dl = DataLoader(\n",
        "    EmotionDataset(train_df, augment_prob=0.2),\n",
        "    batch_size=16,\n",
        "    sampler=train_sampler,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,  # GPU: good to use pinned memory\n",
        ")\n",
        "\n",
        "val_dl = DataLoader(\n",
        "    EmotionDataset(val_df, augment_prob=0.0),\n",
        "    batch_size=16,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "test_dl = DataLoader(\n",
        "    EmotionDataset(test_df, augment_prob=0.0),\n",
        "    batch_size=16,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# MODEL (MiniLM-based Regressor)\n",
        "# ============================================================\n",
        "class TransformerRegressor(nn.Module):\n",
        "    def __init__(self, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(MINILM_MODEL_NAME)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.reg_head = nn.Linear(self.encoder.config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, ids, mask):\n",
        "        out = self.encoder(ids, attention_mask=mask)\n",
        "        # Use CLS token representation (position 0)\n",
        "        pooled = out.last_hidden_state[:, 0, :]\n",
        "        pooled = self.dropout(pooled)\n",
        "        return self.reg_head(pooled).view(-1)\n",
        "\n",
        "# ============================================================\n",
        "# EVALUATION FUNCTION (NORMAL + ORIGINAL SCALE)\n",
        "# ============================================================\n",
        "def evaluate(model: nn.Module, loader: DataLoader, qt: QuantileTransformer):\n",
        "    model.eval()\n",
        "    mse_loss = nn.MSELoss()\n",
        "\n",
        "    batch_losses = []\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Validating\", ncols=100):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            with autocast(enabled=use_amp):\n",
        "                out = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
        "                loss = mse_loss(out, batch[\"labels\"])\n",
        "\n",
        "            batch_losses.append(loss.item())\n",
        "            all_preds.extend(out.detach().cpu().numpy())\n",
        "            all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
        "\n",
        "    preds = np.array(all_preds).reshape(-1, 1)\n",
        "    labels = np.array(all_labels).reshape(-1, 1)\n",
        "\n",
        "    # Normalized metrics (z-space)\n",
        "    mse_norm = mean_squared_error(labels, preds)\n",
        "    spear_norm = spearmanr(preds.ravel(), labels.ravel()).correlation\n",
        "\n",
        "    # Back to original arousal scale\n",
        "    preds_orig = qt.inverse_transform(preds).ravel()\n",
        "    labels_orig = qt.inverse_transform(labels).ravel()\n",
        "\n",
        "    mse_orig = mean_squared_error(labels_orig, preds_orig)\n",
        "    spear_orig = spearmanr(preds_orig, labels_orig).correlation\n",
        "\n",
        "    return {\n",
        "        \"avg_loss\": float(np.mean(batch_losses)),\n",
        "        \"mse_norm\": float(mse_norm),\n",
        "        \"spear_norm\": float(spear_norm),\n",
        "        \"mse_orig\": float(mse_orig),\n",
        "        \"spear_orig\": float(spear_orig),\n",
        "    }\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING LOOP (with early stopping)\n",
        "# ============================================================\n",
        "hp = {\"lr\": 3e-5, \"dropout\": 0.05}\n",
        "max_epochs = 15\n",
        "patience_limit = 5\n",
        "\n",
        "\n",
        "def train_model(hp):\n",
        "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "    model = TransformerRegressor(dropout=hp[\"dropout\"]).to(device)\n",
        "\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=hp[\"lr\"])\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=3, gamma=0.5)\n",
        "    mse_loss = nn.MSELoss()\n",
        "\n",
        "    best_val = float(\"-inf\")\n",
        "    patience = 0\n",
        "    ckpt_path = \"checkpoints/best_single_model.pt\"\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"ðŸ”¥ TRAINING MODEL\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for batch in tqdm(train_dl, desc=f\"Epoch {epoch}\", ncols=100):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            with autocast(enabled=use_amp):\n",
        "                out = model(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
        "                loss = mse_loss(out, batch[\"labels\"])\n",
        "\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optim.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        val_metrics = evaluate(model, val_dl, qt)\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d} | \"\n",
        "            f\"TrainLoss={total_loss:.4f} | \"\n",
        "            f\"ValLoss={val_metrics['avg_loss']:.5f} | \"\n",
        "            f\"ValMSE_norm={val_metrics['mse_norm']:.5f} | \"\n",
        "            f\"ValMSE_orig={val_metrics['mse_orig']:.5f} | \"\n",
        "            f\"Spearman_orig={val_metrics['spear_orig']:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Early stopping based on original-scale MSE\n",
        "        if val_metrics[\"spear_orig\"] > best_val:\n",
        "            best_val = val_metrics[\"spear_orig\"]\n",
        "            patience = 0\n",
        "            torch.save(model.state_dict(), ckpt_path)\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience >= patience_limit:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# RUN TRAINING\n",
        "# ============================================================\n",
        "model = train_model(hp)\n",
        "\n",
        "# Load best state\n",
        "best_ckpt_path = \"checkpoints/best_single_model.pt\"\n",
        "model.load_state_dict(torch.load(best_ckpt_path, map_location=device))\n",
        "\n",
        "# Save artifacts\n",
        "torch.save(model.state_dict(), \"best_model.pt\")\n",
        "tokenizer.save_pretrained(\"best_model_tokenizer\")\n",
        "joblib.dump(qt, \"quantile_transformer.pkl\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ Saved best_model.pt + tokenizer + quantile transformer\")\n",
        "\n",
        "# ============================================================\n",
        "# FINAL TEST EVAL\n",
        "# ============================================================\n",
        "print(\"\\n================ TEST RESULTS ================\")\n",
        "test_metrics = evaluate(model, test_dl, qt)\n",
        "print(\"TEST MSE (norm):   \", test_metrics[\"mse_norm\"])\n",
        "print(\"TEST Spearman (norm):\", test_metrics[\"spear_norm\"])\n",
        "print(\"TEST MSE (orig):   \", test_metrics[\"mse_orig\"])\n",
        "print(\"TEST Spearman (orig):\", test_metrics[\"spear_orig\"])\n",
        "\n",
        "# ============================================================\n",
        "# BASELINE TF-IDF\n",
        "# ============================================================\n",
        "print(\"\\nRunning TF-IDF baseline...\")\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_train = tfidf.fit_transform(train_df[\"text\"])\n",
        "X_test = tfidf.transform(test_df[\"text\"])\n",
        "\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X_train, train_df[\"arousal_norm\"])\n",
        "baseline_preds = ridge.predict(X_test)\n",
        "\n",
        "baseline_mse = mean_squared_error(test_df[\"arousal_norm\"], baseline_preds)\n",
        "print(\"Baseline TF-IDF MSE (norm space):\", baseline_mse)\n",
        "\n",
        "joblib.dump(tfidf, \"tfidf_vectorizer.pkl\")\n",
        "print(\"Saved TF-IDF vectorizer â†’ tfidf_vectorizer.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPxMy8USMFkW",
        "outputId": "97fa5a5c-9935-4c6b-a055-e3e490def7c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "___ Example 1 ___\n",
            "Text: The sun rose slowly over the quiet lake. Birds chirped softly in the distance, and the water was still, reflecting the pale morning sky. A gentle breeze rustled the reeds at the waterâ€™s edge as the day began in peaceful silence.\n",
            "Predicted arousal (normalized scale): -1.7065761089324951\n",
            "Predicted arousal (original scale): 0.2739526669822491\n",
            "\n",
            "___ Example 2 ___\n",
            "Text: She strolled through the empty park in the early autumn afternoon. Leaves drifted lazily from the trees, and the scent of damp earth mixed with faint traces of wood smoke from distant chimneys. The air was cool, and the world felt calm and unhurried.\n",
            "Predicted arousal (normalized scale): -0.2581581473350525\n",
            "Predicted arousal (original scale): 0.5171437196911505\n",
            "\n",
            "___ Example 3 ___\n",
            "Text: He opened the letter and his hands trembled. The words inside were unexpected: absence, regret, final decisions. His chest tightened as he read each line slowly, feeling a strange mix of sorrow and lingering hope as the paper slipped from his grasp.\n",
            "Predicted arousal (normalized scale): 0.10453380644321442\n",
            "Predicted arousal (original scale): 0.529850947052626\n",
            "\n",
            "___ Example 4 ___\n",
            "Text: Rain battered the windows, echoing in the hollow silence of the room. Thunder rolled overhead. She paced back and forth, heart thudding in her ears, shadows dancing across the walls. With every flash of lightning, memories she thought forgotten surged up â€” sharp, raw, and insistent.\n",
            "Predicted arousal (normalized scale): 0.7738974690437317\n",
            "Predicted arousal (original scale): 0.5573961895482559\n",
            "\n",
            "___ Example 5 ___\n",
            "Text: Smoke and fire roared around them, the building collapsing with a deafening crash. She reached out, screaming for help, her breath ragged and wild. Splinters and rubble rained down as panic clawed at her mind â€” every instinct screaming to run, but the world blurred into chaos.\n",
            "Predicted arousal (normalized scale): 1.6774671077728271\n",
            "Predicted arousal (original scale): 0.5998143192132742\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#QUALITATIVE TESTING\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import joblib\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# === 1. Reconstruct the full model definition exactly as in training ===\n",
        "class TransformerRegressor(torch.nn.Module):\n",
        "    def __init__(self, model_name=\"nreimers/MiniLM-L6-H384-uncased\", dropout=0.05):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.reg_head = torch.nn.Linear(self.encoder.config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, ids, mask):\n",
        "        out = self.encoder(ids, attention_mask=mask)\n",
        "        pooled = out.last_hidden_state[:, 0, :]\n",
        "        pooled = self.dropout(pooled)\n",
        "        return self.reg_head(pooled).view(-1)\n",
        "\n",
        "# === 2. Load model + tokenizer + quantile transformer ===\n",
        "model = TransformerRegressor()\n",
        "model.load_state_dict(torch.load(\"best_model.pt\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"best_model_tokenizer\")\n",
        "qt = joblib.load(\"quantile_transformer.pkl\")\n",
        "\n",
        "# === 3. Define your example texts (low â†’ high arousal) ===\n",
        "texts = [\n",
        "    # 1. Low arousal\n",
        "    \"The sun rose slowly over the quiet lake. Birds chirped softly in the distance, and the water was still, reflecting the pale morning sky. A gentle breeze rustled the reeds at the waterâ€™s edge as the day began in peaceful silence.\",\n",
        "    # 2. Lowâ€“medium arousal\n",
        "    \"She strolled through the empty park in the early autumn afternoon. Leaves drifted lazily from the trees, and the scent of damp earth mixed with faint traces of wood smoke from distant chimneys. The air was cool, and the world felt calm and unhurried.\",\n",
        "    # 3. Medium arousal\n",
        "    \"He opened the letter and his hands trembled. The words inside were unexpected: absence, regret, final decisions. His chest tightened as he read each line slowly, feeling a strange mix of sorrow and lingering hope as the paper slipped from his grasp.\",\n",
        "    # 4. Mediumâ€“high arousal\n",
        "    \"Rain battered the windows, echoing in the hollow silence of the room. Thunder rolled overhead. She paced back and forth, heart thudding in her ears, shadows dancing across the walls. With every flash of lightning, memories she thought forgotten surged up â€” sharp, raw, and insistent.\",\n",
        "    # 5. High arousal\n",
        "    \"Smoke and fire roared around them, the building collapsing with a deafening crash. She reached out, screaming for help, her breath ragged and wild. Splinters and rubble rained down as panic clawed at her mind â€” every instinct screaming to run, but the world blurred into chaos.\",\n",
        "]\n",
        "\n",
        "# === 4. Inference loop over all texts ===\n",
        "for i, text in enumerate(texts, start=1):\n",
        "    enc = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        norm_pred = model(enc[\"input_ids\"], enc[\"attention_mask\"]).cpu().item()\n",
        "    orig_pred = qt.inverse_transform([[norm_pred]])[0, 0]\n",
        "\n",
        "    print(f\"___ Example {i} ___\")\n",
        "    print(\"Text:\", text)\n",
        "    print(\"Predicted arousal (normalized scale):\", norm_pred)\n",
        "    print(\"Predicted arousal (original scale):\", orig_pred)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No8bWA8IMTmB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
